---
title: "Linear regression in R"
author: "Erin Shellman"
date: "April 13 - 27, 2015"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    theme: readable
    toc: yes
    toc_depth: 3
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
require(dplyr)
require(ggplot2)
require(GGally)
require(scales)
require(lubridate)
require(caret)

setwd('~/projects/BI-TECH-CP303/projects/project 1')
usage = read.delim('./data/usage_2012.tsv',
                   sep = '\t',
                   header = TRUE)

stations = read.delim('./data/stations.tsv',
                   sep = '\t',
                   header = TRUE)

weather = read.delim('./data/daily_weather.tsv',
                   sep = '\t',
                   header = TRUE)
```

## Linear regression 

In this tutorial we'll learn:

* how to `merge` datasets
* how to fit linear regression models
* how to split data into test and train sets
* how to tune our models and select features

### Data preparation

We're working with the Capital Bikeshare again this week, so start by reading in
*usage*, *weather*, *stations*.
```{r, eval = FALSE}
library(dplyr)
library(ggplot2)
library(lubridate)

usage = read.delim('usage_2012.tsv',
                   sep = '\t',
                   header = TRUE)

weather = read.delim('daily_weather.tsv',
                   sep = '\t',
                   header = TRUE)

stations = read.delim('stations.tsv',
                   sep = '\t',
                   header = TRUE)
```

### Merging data

We have three related datasets to work with, but we can't really get started 
until they're combined. Let's start with *usage* and *weather*.
The *usage* dataframe is at the resolution of the hour, while the *weather* data
are at the resolution of a day, so we know we're going to have to either 
duplicate or compress data to merge. I vote compress, let's summarize! 
```{r}
head(usage)
custs_per_day = 
  usage %>% 
    group_by(time_start = as.Date(time_start), station_start, cust_type) %>% 
    summarize(no_rentals = n(),
              duration_mins = mean(duration_mins, na.rm = TRUE))

head(custs_per_day)
```

Perfection, now we can merge!  What's the key?
```{r}
# make sure we have consistent date formats
custs_per_day$time_start = ymd(custs_per_day$time_start)
weather$date = ymd(weather$date)

# then merge. see ?merge for more details about the function
weather_rentals = merge(custs_per_day, weather, 
                        by.x = 'time_start', by.y = 'date')

# check dimensions after to make sure they are what you expect
dim(custs_per_day)
dim(weather)
dim(weather_rentals)

head(weather_rentals)
```

Great, now we want to merge on the last dataset, *stations*. What is the key to 
link *weather_rentals* with *stations*?
```{r}
final_data = merge(weather_rentals, stations, 
                   by.x = 'station_start', by.y = 'station')
dim(final_data)
dim(weather_rentals)

head(final_data[, 1:30])

# probably want to save this now!
write.table(final_data, 
            'bikeshare_modeling_data.tsv', 
            row.names = FALSE, sep = '\t')

# rename to something more convenient and remove from memory
data = final_data
rm(final_data)
```

### The `lm()` function

The function for creating a linear model in R is `lm()` and the primary 
arguments are *formula* and *data*. Formulas in R are a little funny,
instead of an = sign, they are expressed with a ~. Let's fit the model we saw in
the lecture notes: $rentals = \beta_0 + \beta_1*crossing$. There's a little snag 
we have to take care of first. Right now we've got repeated measures *i.e.* 
one measurement per day, so we need to aggregate again this time over date.
```{r}
rentals_crossing = 
  data %>% 
    group_by(station_start) %>% 
    summarize(mean_rentals = mean(no_rentals),
              crossing = mean(crossing))

head(rentals_crossing)

# plot it
ggplot(rentals_crossing, aes(x = crossing, y = mean_rentals)) +
  geom_smooth(method = 'lm', size = 2) +
  geom_point(size = 4, alpha = 0.60) +
  theme_minimal()

model = lm(mean_rentals ~ crossing, data = rentals_crossing)

# view what is returned in the lm object
attributes(model)

# get model output
summary(model)

# print model diagnostics
par(mfrow = c(2, 2))
plot(model)
```

The `attributes()` function can be called on just about any object in R and it
returns a list of all the things inside. It's a great way to explore 
objects and see what values are contained inside that could be used in other 
analysis. For example, extracting the residuals via `model$residuals` is useful
if we want to print diagnostic plots like those above.

When we run `summary()` on the `lm` object, we see the results. The *Call*
section just prints back the model specification, and the *Residuals* section
contains a summary of the distribution of the errors. The fun stuff is in the
*Coefficients* section. In the first row contains the covariate names followed 
by their estimates, standard errors, t- and p-values. Our model ends up being 
`rentals = 15 + 0.24(crosswalks)` which means that the average number of rentals
when there are no crosswalks is 15, and the average increases by 1 rental for
every four additional crosswalks.

We can fit regressions with multiple covariates the same way:
```{r}
# lets include windspeed this time
rentals_multi = 
  data %>% 
    group_by(station_start) %>% 
    summarize(mean_rentals = mean(no_rentals),
              crossing = mean(crossing),
              windspeed = mean(windspeed))

head(rentals_multi)

ggplot(rentals_multi, aes(x = windspeed, y = mean_rentals)) +
  geom_smooth(method = 'lm', size = 2) +
  geom_point(size = 4, alpha = 0.60) +
  theme_minimal()

model = lm(mean_rentals ~ crossing + windspeed, data = rentals_multi)
summary(model)
```

The model coefficients changed quite a lot when we added in wind speed. The 
intercept is now negative, and the wind speed coefficient is huge! When 
interpreting coefficients, it's important to keep the scale in mind. Wind speed 
ranges from 0.05 to 0.44 so when you multiply 1172 by 0.05 for example, you end 
up with about 60, which is within the range we'd expect.

Let's try one more, this time we'll include a factor variable.
```{r}
rentals_multi = 
  data %>% 
    group_by(station_start, is_holiday) %>% 
    summarize(mean_rentals = mean(no_rentals),
              crossing = mean(crossing),
              windspeed = mean(windspeed))

head(rentals_multi)

# plot crossings, colored by is_holiday
ggplot(rentals_multi, 
       aes(x = crossing, y = mean_rentals, color = factor(is_holiday))) +
  geom_smooth(method = 'lm', size = 2) +
  geom_point(size = 4, alpha = 0.60) +
  theme_minimal()

# plot windspeed, colored by is_holiday
ggplot(rentals_multi, 
       aes(x = windspeed, y = mean_rentals, color = factor(is_holiday))) +
  geom_smooth(method = 'lm', size = 2) +
  geom_point(size = 4, alpha = 0.60) +
  theme_minimal()

model = lm(mean_rentals ~ crossing + windspeed + factor(is_holiday), 
           data = rentals_multi)
summary(model)
```

The output looks a little funny now. There's a term called 
`factor(is_holiday)1`, what does that mean? Factors are category variables and 
their interpretation is relative to a baseline. Our factor `is_holiday` 
only has two levels, 0 and 1, and R sets 0 to the baseline by default. So the 
interpretation of that term is that we can expect about 10 additional rentals 
when it is a holiday (*i.e.* `is_holiday == 0`) and the other variables are
fixed.

## The *caret* package

```{r, include = FALSE}
rm(data)
data = read.delim('./data/final_modeling_data.tsv', sep = '\t', header = TRUE)
data$weekday = factor(data$weekday, labels = 0:6, levels = 0:6)
data$season_code = factor(data$season_code)
datais_holiday = factor(data$is_holiday)
data$weather_code = factor(data$weather_code)
```

For this section, we'll use the fully cleaned and combined data from the 
[project-1-data-cleanup](https://github.com/erinshellman/BI-TECH-CP303/blob/master/projects/project%201/project-1-data-cleanup.Rmd) 
file, so make sure you've gone through and cleaned
your data up like that, or download the clean file from 
[here](https://s3-us-west-2.amazonaws.com/bi-tech-cp303/project+1/capital-bike-share/final_modeling_data.tsv).

```{r, eval = FALSE}
data = read.delim('final_modeling_data.tsv', sep = '\t', header = TRUE)
```

We'll be using the *caret* package (short for **c**lassification **a**nd 
**re**gression **t**raining) for model development because it integrates
many modeling packages in R into one unified syntax. That means more reusable
code for us! *caret* contains helper functions that provide a unified 
framework for data cleaning/splitting, model training, and comparison. I highly
recommend the 
[optional reading](https://github.com/erinshellman/BI-TECH-CP303/blob/master/reading/regression/v28i05.pdf)
this week which provides a great overview of the *caret* package.

```{r, eval = FALSE}
install.packages('caret', dependencies = TRUE)
library(caret)

set.seed(1234) # set a seed
```

Setting a seed in R insures that you get identical results each time you run
your code. Since resampling methods are inherently probabilistic, every time we 
rerun them we'll get slightly different answers. Setting the seed to the same 
number insures that we get identical randomness each time the code is run, and
that's helpful for debugging.

### Train and test data

Before any analysis in this class we'll need to divide our data into train and 
test sets. Check out 
[this](http://topepo.github.io/caret/training.html) nice overview for more 
details. The *training* set is typically about 75% of the data and is used for 
all the model development. Once we have a model we're satisfied with, we use our
*testing* set, the other 25% to generate model predictions. Splitting the data
into the two groups, train and test, generates two types of errors, in-sample 
and out-of-sample errors. *In-sample* errors are the errors derived from same 
data the model was built with. *Out-of-sample* errors are derived from measuring 
the error on a fresh data set. We are interested in the out-of-sample error 
because this quantity represents how'd we'd expect the model to perform in the
future on brand new data.

Here's how to split the data with *caret*:
```{r}
# select the training observations
in_train = createDataPartition(y = data$rentals,
                                   p = 0.75, # 75% in train, 25% in test
                                   list = FALSE)
head(in_train) # row indices of observations in the training set

train = data[in_train, ]
test = data[-in_train, ]

dim(train)
dim(test)
```

Note: I recommend doing all data processing and aggregation steps *before* 
splitting out your train/test sets.

### Training

Our workhorse function in the *caret* package in the `train` function. This
function can be used to evaluate performance parameters, choose optimal models 
based on the values of those parameters, and estimate model performance. For 
regression we can use it in place of the `lm()` function. Here's our last
regression model using the train function.

```{r}
model_fit = train(rentals ~ crossing + windspeed + factor(is_holiday), 
                  data = train, 
                  method = 'lm',
                  metric = 'RMSE') 
print(model_fit)
summary(model_fit)

# get predictions
out_of_sample_predictions = predict(model_fit, newdata = test)

# compare predictions against the observed values
errors = data.frame(predicted = out_of_sample_predictions,
                    observed = test$rentals,
                    error = out_of_sample_predictions - test$rentals)

# eh, not so good
ggplot(data = errors, aes(x = predicted, y = observed)) + 
  geom_abline(aes(intercept = 0, slope = 1), 
              size = 3, alpha = 0.70, color = 'red') +
  geom_point(size = 3, alpha = 0.80) +
  ggtitle('out-of-sample errors') +
  theme_minimal()
```

Our prediction accuracy is not so great for this model. The in-sample RMSE is 
about 27 which means that on average the predictions are off by about 27 
rentals. Let's fit the giant model we made before:
```{r}
full_model = train(rentals ~ ., 
                  data = train, 
                  method = 'lm') 
```

The in-sample RMSE is about 19, so definitely an improvement over the previous 
model, but this model is really complex and probably not going to be usable by
Pronto. How can we reduce the complexity of the model, but maintain reasonable
predictive accuracy?

### Preprocessing

Shrinkage methods require that the predictors are normalized to be on the same 
scale. We can accomplish this by centering and scaling the data. You center a 
variable by subtracting the mean of the variable from from each observation. To
scale your observations you then divide the centered observation by the variable
standard deviation. Now the variable follows a standard normal distribution with
mean = 0 and standard deviation = 1.

The *caret* package has lots of convenient functions for 
[preprocessing data](http://topepo.github.io/caret/preprocess.html), check 'em 
out!

#### Converting factors to dummy variables

We run into some trouble if we try to just center and scale the data because its
got factor variables and you can't subtract a number from a category. We can 
use the `model.matrix` function to fix that really quickly.

```{r, warning = FALSE}
no_factors = as.data.frame(model.matrix(rentals ~ . -1, data = data))

# put rentals back on
no_factors$rentals = data$rentals

full_model_scaled = train(rentals ~ ., 
                  data = no_factors, 
                  method = 'lm',
                  preProcess = c('center', 'scale'))
```

Coefficients estimated with normalized data have a different interpretation than
coefficients from un-normalized data. In this case when the data are scaled the 
intercept has a better interpretation, it's the expected number of rentals when 
all the predictors are at their average value. So, in this case, when all the 
predictors are at their average values, we expect about 21 rentals per day. 
In the previous full-model we had an intercept of about -28, which could be 
interpreted as the expected number of rentals when all the other predictors 
have a value of 0. That's pretty unsatisfying for a couple reasons. First, we 
can't have negative rentals! Second, for a lot of the predictors it doesn't make 
sense to plug in 0's. What does it mean to have a duration of 0? Or a temp of 0?
Centering and scaling fix the non-interpret ability of the previous models.

Since we divide by the standard deviation during scaling, the non-intercept 
coefficients in the centered and scaled model can be interpreted as the 
increase in $y$ associated with a 1 standard deviation increase in $x$.

## Model Selection

### Variable combination

A simple method to reduce model complexity is to combine some of the variables. 
For example the dataset contains a variable for *alcohol*, *pub* and *bar*, 
likewise there's a variable for *food_court*,
*restaurant*, *food_cart*, and *fast_food*. Maybe we can retain information
and remove some variables.

```{r, warning = FALSE}
no_factors$food = no_factors$fast_food + no_factors$restaurant + 
  no_factors$food_court + no_factors$bar.restaurant + 
  no_factors$cafe + no_factors$food_cart 

no_factors$nightlife = no_factors$bar + no_factors$club + 
  no_factors$pub + no_factors$nightclub 

no_factors$seedy_stuff = no_factors$stripclub + no_factors$strip_club + 
  no_factors$alcohol + no_factors$check_cashing + no_factors$motel + 
  no_factors$hostel

no_factors$tourism = no_factors$theatre + no_factors$arts_centre + 
  no_factors$tourist + no_factors$school..historic. + no_factors$hotel + 
  no_factors$gallery + no_factors$artwork + no_factors$sculpture + 
  no_factors$museum + no_factors$tour_guide + no_factors$car_rental + 
  no_factors$guest_house + no_factors$landmark + no_factors$attraction + 
  no_factors$information

dim(no_factors)

# now remove those variables from the no_factorsset
no_factors = 
  no_factors %>%
  select(-fast_food, -restaurant, -food_court, -bar.restaurant, -cafe, 
         -food_cart, -bar, -club, -pub, -nightclub, -stripclub, -strip_club, 
         -alcohol, -check_cashing, -motel, -hostel, -theatre, -arts_centre, 
         -tourist, -school..historic., -hotel, -gallery, -artwork, -sculpture, 
         -museum, -tour_guide, -car_rental, -guest_house, -landmark, 
         -attraction, -information)

# Reduced the dataset by 31 variables!
dim(no_factors)
```

Try out your own categories, these are just a few to get you started.
We'll learn how to make categories computationally when we cover clustering.

We've change the dataframe, don't forget to redefine the train and test sets!
```{r, warning = FALSE}
train = no_factors[in_train, ]
test = no_factors[-in_train, ]

dim(train)
dim(test)

# how does our new full-model compare?
full_model = train(rentals ~ ., 
                  data = train, 
                  method = 'lm') 
```

### Subset selection

We haven't talked much about computational limitations yet, but it's a good 
time to start. Selection methods can be *extremely* slow. Why? Because we have 
$2^p = 2^{117}$ possible variable combinations. I recommend
doing some combining before trying these methods. I'll leave the combining 
up to you, but to make sure these models can run in less than infinite time,
I'm going to remove a bunch of predictors so you get the idea.

```{r, warning = FALSE}
train = 
  train %>% 
    select(rentals, cust_typeCasual, cust_typeRegistered, cust_typeSubscriber, 
           weekday1, weekday2, weekday3, weekday4, weekday5, weekday6, 
           season_code2, season_code3, season_code4, is_holiday, weather_code2, 
           weather_code3, humidity, windspeed, temp, duration, food, nightlife, 
           seedy_stuff, tourism) 

test = 
  test %>% 
    select(rentals, cust_typeCasual, cust_typeRegistered, cust_typeSubscriber, 
           weekday1, weekday2, weekday3, weekday4, weekday5, weekday6, 
           season_code2, season_code3, season_code4, is_holiday, weather_code2, 
           weather_code3, humidity, windspeed, temp, duration, food, nightlife, 
           seedy_stuff, tourism)

# forward selection
forward_model = train(rentals ~ ., 
                      data = na.omit(train),  
                      method = 'leapForward',
                      preProcess = c('center', 'scale'),
                      # try models of size 1 - 23
                      tuneGrid = expand.grid(nvmax = 1:23),
                      trControl = trainControl(method = 'cv', number = 5)) 

# what does this return?
attributes(forward_model)

# what what should the number of variables, k, be?
forward_model$bestTune

# what metric was used?
forward_model$metric

# here's a handful of other useful plots and summaries
print(forward_model)
summary(forward_model)
plot(forward_model)
plot(varImp(forward_model))

# compare all the models
plot(forward_model$finalModel, scale = 'adjr2')

# backward_selection
backward_model = train(rentals ~ ., 
                       data = na.omit(train),  
                       method = 'leapBackward',
                       preProcess = c('center', 'scale'),
                       tuneGrid = expand.grid(nvmax = 1:23),
                       trControl = trainControl(method = 'cv', number = 5)) 

plot(backward_model)
plot(backward_model$finalModel, scale = 'adjr2')
plot(varImp(backward_model, scale = TRUE))

# steps in both directions
hybrid_model = train(rentals ~ ., 
                     data = na.omit(train),  
                     method = 'leapSeq',
                     preProcess = c('center', 'scale'),
                     tuneGrid = expand.grid(nvmax = 1:23),
                     trControl = trainControl(method = 'cv', number = 5)) 

plot(hybrid_model)
plot(hybrid_model$finalModel, scale = 'adjr2')
plot(varImp(hybrid_model))
```

### Shrinkage

#### Ridge regression 

```{r}
# ridge regression
ridge_model = train(rentals ~ ., 
                    data = train, 
                    method = 'ridge',
                    preProcess = c('center', 'scale'),
                    tuneLength = 10,
                    # reducing the cv for speed
                    trControl = trainControl(method = 'cv', number = 5))

print(ridge_model)
plot(ridge_model)
plot(ridge_model$finalModel)
plot(varImp(ridge_model))

# get the coefficients for the model
# NOTE: shrinkage methods don't have intercept terms
ridge_coefs = predict(ridge_model$finalModel, type = 'coef', mode = 'norm')$coefficients

# ridge regression with variable selection
ridge_model2 = train(rentals ~ ., 
                     data = train, 
                     method = 'foba',
                     preProcess = c('center', 'scale'),
                     tuneLength = 10,
                     trControl = trainControl(method = 'cv', number = 5))

print(ridge_model2)
plot(ridge_model2)
plot(varImp(ridge_model2))
```

Selection, ridge regression, and lasso are just a couple techniques at our
disposal for decreasing our model size. See 
[this page](http://topepo.github.io/caret/Feature_Selection_Wrapper.html) for
a list of other available options to try out if you like.

#### Lasso 
```{r}
lasso_model = train(rentals ~ ., 
                    data = na.omit(train),
                    method = 'lasso',
                    preProc = c('scale', 'center'),
                    tuneLength = 10,
                    trControl = trainControl(method = 'cv', number = 5))

print(lasso_model)
plot(lasso_model)
plot(varImp(lasso_model))
plot(lasso_model$finalModel)

# get the model coefficients
lasso_coefs = predict(lasso_model$finalModel, type = 'coef', mode = 'norm')$coefficients
```

## Measuring predictive accuracy

All right, now we've got a nice collection of models. Which one should we 
report?

```{r}
results = resamples(list(forward_selection = forward_model, 
                               backward_selection = backward_model, 
                               hybrid_selection = hybrid_model,
                               ridge_regression = ridge_model,
                               lasso_regeression = lasso_model))

# compare RMSE and R-squared
summary(results)

# plot results
dotplot(results)
```

Those are in-sample statistics however, so if we want to compare the model's
out-of-sample prediction accuracy, we need to compute the RMSE using the test
data we held out. Let's compare two models: backward selection and 
lasso:
```{r}
backward_predictions = predict(backward_model, test)
sqrt(mean((backward_predictions - test$rentals)^2 , na.rm = TRUE))

lasso_predictions = predict(lasso_model, test)
sqrt(mean((lasso_predictions - test$rentals)^2 , na.rm = TRUE))

```

## Project tips

Check out this list of different model selection methods and try a couple out.

  * How do they work?
  * Which works best?
  
Once you've spent some time exploring candidate models, pick one and use it in
your report.