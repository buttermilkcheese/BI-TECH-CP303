---
title: "Logistic regression in R"
author: "Erin Shellman"
date: "May 04, 2015"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    theme: readable
    toc: yes
    toc_depth: 3
---

```{r setup, include = FALSE}
# my set-up
knitr::opts_chunk$set(cache = TRUE)
require(dplyr)
require(ggplot2)
require(GGally)
require(scales)
require(lubridate)
require(caret)
setwd('~/projects/BI-TECH-CP303/projects/project 2')
data = read.delim('./data/bot_or_not.tsv',
                   sep = '\t',
                   header = TRUE)
```

For the second project we'll explore user data from Twitter. The data set has a
variable called `bot` that denotes whether the user is a bot, and then some 
features about the user account.

```{r, eval = FALSE}
library(dplyr)
library(ggplot2)
library(lubridate)
library(GGally)
library(scales)
library(caret)

data = read.delim('bot_or_not.tsv',
                   sep = '\t',
                   header = TRUE)
```

## Exploratory data analysis

We've got a brand new dataset here, so let's familiarize ourselves by
conducting an exploratory data analysis. First, let's make sure the variables 
are of the right types, *i.e.* continuous variables are continuous and dummy 
variables are factors.

```{r}
# tell R which variables are categorical (factors)
data$bot = factor(data$bot)
data$default_profile = factor(data$default_profile)
data$default_profile_image = factor(data$default_profile_image)
data$geo_enabled = factor(data$geo_enabled)
data$profile_background_tile = factor(data$profile_background_tile)
data$verified = factor(data$verified)
```

Now, we can explore the data a bit.
```{r, warning = FALSE, message = FALSE}
summary(data)

# inspect the trends
ggpairs(data[ , c('followers_count', 'friends_count', 'account_age', 
                  'days_since_last_tweet', 'listed_count', 
                  'status_favorite_count', 'bot')],
        lower = list(continuous = 'points', params = list(alpha = 0.70)),
        diag = list(continuous = 'density', params = list(alpha = 0.70)), 
        upper = list(continuous = 'cor'),
        axisLabels = 'show', color = 'bot')

ggplot(data, aes(x = followers_count, fill = factor(bot))) +
  geom_histogram(alpha = 0.60)

# whoa, some people have a lot of followers, but most don't. we need to lob off
# the long tail so we can see the distribution better
ggplot(filter(data, followers_count < 100), 
       aes(x = followers_count, fill = factor(bot))) +
  geom_histogram(alpha = 0.60)

# how about the number of people they follow?
ggplot(data, aes(x = friends_count, fill = factor(bot))) +
  geom_histogram(alpha = 0.60)

# it's a little hard to see
ggplot(filter(data, friends_count < 100), 
       aes(x = friends_count, fill = factor(bot))) +
  geom_histogram(alpha = 0.60)

# that's better
ggplot(filter(data, friends_count < 2500), 
       aes(x = friends_count, fill = factor(bot))) +
  geom_density(alpha = 0.60)

# what about account age?
ggplot(data, aes(x = account_age, fill = factor(bot))) +
  geom_density(alpha = 0.60)

# geo enabled?
xtabs(~bot + geo_enabled, data = data)
```

## Logistic regression

Lucky for us, we can use the handy `train()` function for fitting logistic 
regressions.

```{r, warning = FALSE, message = FALSE}
set.seed(243)
data = na.omit(data)

# select the training observations
in_train = createDataPartition(y = data$bot,
                               p = 0.75, # 75% in train, 25% in test
                               list = FALSE)

train = data[in_train, ]
test = data[-in_train, ]

# drop the ids
train$id = NULL
test$id = NULL
```

Check out [this page](http://topepo.github.io/caret/Logistic_Regression.html)
for more types of logistic regression to try out.

```{r, warning = FALSE}
logistic_model = train(bot ~ ., 
                       data = na.omit(train),  
                       method = 'glm',
                       family = binomial,
                       preProcess = c('center', 'scale'))

summary(logistic_model)
plot(varImp(logistic_model))

# test predictions
logistic_predictions = predict(logistic_model, newdata = test)
confusionMatrix(logistic_predictions, test$bot)

# stepwise logisitic regression
step_model = train(bot ~ ., 
                   data = na.omit(train),  
                   method = 'glmStepAIC',
                   family = binomial,
                   preProcess = c('center', 'scale'))
summary(step_model)
plot(varImp(step_model))

step_predictions = predict(step_model, newdata = test)
confusionMatrix(step_predictions, test$bot)

# compare
results = resamples(list(logistic_model = logistic_model, 
                         step_model = step_model))

# compare accuracy and kappa
summary(results)

# plot results
dotplot(results)
```

Now that we have a discrete binary outcome we'll use performance metrics called
accuracy and kappa instead of $R^2$ and RMSE. 